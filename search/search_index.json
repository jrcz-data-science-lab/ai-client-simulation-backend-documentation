{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"General overview of the workings . \u251c\u2500\u2500 backend \u2502 \u251c\u2500\u2500 api \u2502 \u251c\u2500\u2500 code \u2502 \u251c\u2500\u2500 custom \u2502 \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 docker-compose.yml \u2502 \u251c\u2500\u2500 docs \u2502 \u251c\u2500\u2500 .env \u2502 \u251c\u2500\u2500 .env.example \u2502 \u251c\u2500\u2500 streaming \u2502 \u2514\u2500\u2500 tts \u251c\u2500\u2500 frontend \u2502 \u251c\u2500\u2500 node_modules \u2502 \u251c\u2500\u2500 package.json \u2502 \u251c\u2500\u2500 package-lock.json \u2502 \u251c\u2500\u2500 public \u2502 \u251c\u2500\u2500 server.js \u2502 \u2514\u2500\u2500 ssl \u251c\u2500\u2500 .gitignore \u2514\u2500\u2500 readme.md This project structure represents a multi-service application, organized into distinct components as follows: api : Contains a Dockerfile, the main Python script ( main.py ), and a requirements.txt file listing dependencies for the API service. code : Another service similar to the API, with its own Dockerfile, main logic script ( main.py ), and requirements.txt for dependencies. data : Stores data or models related to the Ollama service. docker-compose.yml : Defines how these services interact using Docker Compose. streaming : A separate service like API and code, responsible for handling real-time data or streams. text to speech : A service that deals with text to speech from the incoming text. NGINX documentation The NGINX reverse proxy is a key part of the architecture, acting as a traffic manager that directs incoming requests to the appropriate services based on the URL paths. It ensures secure communication, handles WebSocket connections for real-time features, and redirects HTTP traffic to HTTPS for enhanced security. NGINX code NGINX configuration NGINX proxy parameters NGINX proxy configuration Functionality The system facilitates real-time audio transcription through a WebSocket connection from the frontend. As the audio streams, it is transcribed in real-time. Once the user ends the stream, the inputs are concatenated and passed to the API gateway.","title":"General overview of the workings"},{"location":"#general-overview-of-the-workings","text":". \u251c\u2500\u2500 backend \u2502 \u251c\u2500\u2500 api \u2502 \u251c\u2500\u2500 code \u2502 \u251c\u2500\u2500 custom \u2502 \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 docker-compose.yml \u2502 \u251c\u2500\u2500 docs \u2502 \u251c\u2500\u2500 .env \u2502 \u251c\u2500\u2500 .env.example \u2502 \u251c\u2500\u2500 streaming \u2502 \u2514\u2500\u2500 tts \u251c\u2500\u2500 frontend \u2502 \u251c\u2500\u2500 node_modules \u2502 \u251c\u2500\u2500 package.json \u2502 \u251c\u2500\u2500 package-lock.json \u2502 \u251c\u2500\u2500 public \u2502 \u251c\u2500\u2500 server.js \u2502 \u2514\u2500\u2500 ssl \u251c\u2500\u2500 .gitignore \u2514\u2500\u2500 readme.md This project structure represents a multi-service application, organized into distinct components as follows: api : Contains a Dockerfile, the main Python script ( main.py ), and a requirements.txt file listing dependencies for the API service. code : Another service similar to the API, with its own Dockerfile, main logic script ( main.py ), and requirements.txt for dependencies. data : Stores data or models related to the Ollama service. docker-compose.yml : Defines how these services interact using Docker Compose. streaming : A separate service like API and code, responsible for handling real-time data or streams. text to speech : A service that deals with text to speech from the incoming text.","title":"General overview of the workings"},{"location":"#nginx-documentation","text":"The NGINX reverse proxy is a key part of the architecture, acting as a traffic manager that directs incoming requests to the appropriate services based on the URL paths. It ensures secure communication, handles WebSocket connections for real-time features, and redirects HTTP traffic to HTTPS for enhanced security. NGINX code NGINX configuration NGINX proxy parameters NGINX proxy configuration","title":"NGINX documentation"},{"location":"#functionality","text":"The system facilitates real-time audio transcription through a WebSocket connection from the frontend. As the audio streams, it is transcribed in real-time. Once the user ends the stream, the inputs are concatenated and passed to the API gateway.","title":"Functionality"},{"location":"howItWorks/apigateway/","text":"API gateway This FastAPI application allows users to interact with AI models through a variety of endpoints, including those for generating text prompts, managing models, caching conversation history, and handling real-time transcription through WebSockets. The application integrates with the Ollama service, which handles large language models (LLMs). Key Features Conversation Caching : The application stores conversation history in a file for each user, allowing the model to generate more context-aware responses. Model Management : Users can pull, delete, or create models in Ollama using dedicated API endpoints. Real-time Transcription : Transcription functionality is available through WebSocket and HTTP endpoints, enabling audio data to be processed and transcribed in real time. API Endpoints 1. Health Check GET / Description : Verifies that the API is running. Response : \"Apigateway is running\" 2. Generate Prompt POST /api/prompt/ Description : Generates a response from the AI model based on the provided text prompt. Request Body : { \"model\": \"gemma2:2b\", \"text\": \"What is the meaning of life?\", \"username\": \"user123\" } model : The name of the AI model to use (e.g., gemma2:2b ). text : The input text for the AI model to generate a response. username : A unique identifier for the user (used for caching conversation history). Response : ```json { \"model\": \"gemma2:2b\", \"text\": \"The meaning of life is to seek happiness, purpose, and connection with others.\" } **Functionality**: - The model is invoked using the provided prompt. - The AI's response is cached along with the prompt for future context-aware interactions. ### 3\\. Pull Model ```http POST /api/pull/ Description : Pulls a model from the Ollama service, making it available for use. Request Body : { \"model_name\": \"llama3.1\" } Response : { \"status\": \"Model pulled successfully\" } 4. Delete Model POST /api/delete/ Description : Deletes a model from the Ollama service. Request Body : { \"model\": \"llama3.1\" } Response : { \"status\": \"Model deleted successfully\" } 5. Create Custom Model POST /api/create/ Description : Creates a new custom model using a base model and personality traits. Request Body : { \"name\": \"custom-chat\", \"basemodel\": \"gemma2:2b\", \"personality\": \"Friendly and engaging\" } Response : { \"status\": \"Model created successfully\" } 6. WebSocket Proxy for Transcription WebSocket /proxy-transcribe Description : Establishes a WebSocket connection for real-time transcription of audio data. The audio is sent to a streaming service for processing, and the transcribed text is sent back to the client. WebSocket Flow : The client connects to the WebSocket endpoint and sends audio data (along with the username) to the server. The server sends the audio data to an external streaming service for transcription. The transcribed text is sent back to the client. Request : The client sends audio data to the server with a username key. { \"username\": \"user123\", \"audio\": \"<audio_base64>\" // does not to be discribed } Response : { \"username\": \"user123\", \"transcription\": \"Hello, this is a test transcription.\" } 7. Send Transcription via HTTP POST /proxy-send-transcription Description : Sends a transcription request to the streaming service, triggering transcription of audio for a specified user. Request Body : { \"username\": \"user123\" } Response : { \"status\": \"Transcription sent successfully\" } Error Handling : If there's an issue with the request, the server responds with an error message. 8. Get Audio GET /audio/{username} Description : Fetches the audio file for a specific user from an external streaming service. Response : The audio file is streamed as the response, with the correct Content-Type (default: audio/wav ). Conversation Caching The application uses a conversation cache to store and recall past interactions, improving the context for future requests. The cache is updated each time a new prompt is submitted, and the full conversation history is saved to a file for each user. This allows the AI to generate responses that account for previous interactions. How It Works: When a prompt is received, the AI generates a response using the conversation history. The conversation is then stored in a file specific to the user ( conversationMemory/{username}_memory.txt ). The conversation history is formatted and used as context in future requests, allowing the AI to provide more coherent and contextually aware answers. Example of Conversation Cache Format : User: What is the meaning of life? AI: The meaning of life is to seek happiness, purpose, and connection with others. User: Can you elaborate on happiness? AI: Happiness is often found in meaningful relationships, personal growth, and living with purpose. File Storage: The conversation is saved in a file named conversationMemory/{username}_memory.txt (where {username} is the unique identifier). The conversation cache is updated with each new user interaction. Model Invocation The AI models are invoked through the Ollama class, which interacts with the Ollama service. This service manages the large language models (LLMs) and allows the API to generate text based on the provided prompt. llm = Ollama(model=model_name, base_url=\"http://ollama:11434\") Each time a prompt is received, the entire conversation history (stored in previous_convos ) is included as part of the prompt, enabling context-sensitive responses from the model. full_prompt = f'{previous_convos}\\nUser: {prompt_text}\\nAI:' return llm.invoke(\"Continue the conversation like a human\\n\" + full_prompt) Error Handling The application uses standard HTTP error codes for failure scenarios: 400 Bad Request : The request is malformed or missing required fields. 500 Internal Server Error : There is a problem on the server, such as an issue with the Ollama service or external dependencies. Example of error response for an unsuccessful transcription request: { \"error\": \"HTTP error occurred\", \"details\": \"Error details here\" } Summary of Available Endpoints Endpoint Method Description / GET Health check /api/prompt/ POST Generate prompt response /api/pull/ POST Pull a model from Ollama /api/delete/ POST Delete a model from Ollama /api/create/ POST Create a custom model /proxy-transcribe WebSocket Real-time audio transcription /proxy-send-transcription POST Send transcription request /audio/{username} GET Fetch user audio file This API allows seamless interaction with the Ollama LLM service while handling user-specific conversation histories and supporting real-time transcription of audio data.","title":"API gateway"},{"location":"howItWorks/apigateway/#api-gateway","text":"This FastAPI application allows users to interact with AI models through a variety of endpoints, including those for generating text prompts, managing models, caching conversation history, and handling real-time transcription through WebSockets. The application integrates with the Ollama service, which handles large language models (LLMs).","title":"API gateway"},{"location":"howItWorks/apigateway/#key-features","text":"Conversation Caching : The application stores conversation history in a file for each user, allowing the model to generate more context-aware responses. Model Management : Users can pull, delete, or create models in Ollama using dedicated API endpoints. Real-time Transcription : Transcription functionality is available through WebSocket and HTTP endpoints, enabling audio data to be processed and transcribed in real time.","title":"Key Features"},{"location":"howItWorks/apigateway/#api-endpoints","text":"","title":"API Endpoints"},{"location":"howItWorks/apigateway/#1-health-check","text":"GET / Description : Verifies that the API is running. Response : \"Apigateway is running\"","title":"1. Health Check"},{"location":"howItWorks/apigateway/#2-generate-prompt","text":"POST /api/prompt/ Description : Generates a response from the AI model based on the provided text prompt. Request Body : { \"model\": \"gemma2:2b\", \"text\": \"What is the meaning of life?\", \"username\": \"user123\" } model : The name of the AI model to use (e.g., gemma2:2b ). text : The input text for the AI model to generate a response. username : A unique identifier for the user (used for caching conversation history). Response : ```json { \"model\": \"gemma2:2b\", \"text\": \"The meaning of life is to seek happiness, purpose, and connection with others.\" } **Functionality**: - The model is invoked using the provided prompt. - The AI's response is cached along with the prompt for future context-aware interactions. ### 3\\. Pull Model ```http POST /api/pull/ Description : Pulls a model from the Ollama service, making it available for use. Request Body : { \"model_name\": \"llama3.1\" } Response : { \"status\": \"Model pulled successfully\" }","title":"2. Generate Prompt"},{"location":"howItWorks/apigateway/#4-delete-model","text":"POST /api/delete/ Description : Deletes a model from the Ollama service. Request Body : { \"model\": \"llama3.1\" } Response : { \"status\": \"Model deleted successfully\" }","title":"4. Delete Model"},{"location":"howItWorks/apigateway/#5-create-custom-model","text":"POST /api/create/ Description : Creates a new custom model using a base model and personality traits. Request Body : { \"name\": \"custom-chat\", \"basemodel\": \"gemma2:2b\", \"personality\": \"Friendly and engaging\" } Response : { \"status\": \"Model created successfully\" }","title":"5. Create Custom Model"},{"location":"howItWorks/apigateway/#6-websocket-proxy-for-transcription","text":"WebSocket /proxy-transcribe Description : Establishes a WebSocket connection for real-time transcription of audio data. The audio is sent to a streaming service for processing, and the transcribed text is sent back to the client. WebSocket Flow : The client connects to the WebSocket endpoint and sends audio data (along with the username) to the server. The server sends the audio data to an external streaming service for transcription. The transcribed text is sent back to the client. Request : The client sends audio data to the server with a username key. { \"username\": \"user123\", \"audio\": \"<audio_base64>\" // does not to be discribed } Response : { \"username\": \"user123\", \"transcription\": \"Hello, this is a test transcription.\" }","title":"6. WebSocket Proxy for Transcription"},{"location":"howItWorks/apigateway/#7-send-transcription-via-http","text":"POST /proxy-send-transcription Description : Sends a transcription request to the streaming service, triggering transcription of audio for a specified user. Request Body : { \"username\": \"user123\" } Response : { \"status\": \"Transcription sent successfully\" } Error Handling : If there's an issue with the request, the server responds with an error message.","title":"7. Send Transcription via HTTP"},{"location":"howItWorks/apigateway/#8-get-audio","text":"GET /audio/{username} Description : Fetches the audio file for a specific user from an external streaming service. Response : The audio file is streamed as the response, with the correct Content-Type (default: audio/wav ).","title":"8. Get Audio"},{"location":"howItWorks/apigateway/#conversation-caching","text":"The application uses a conversation cache to store and recall past interactions, improving the context for future requests. The cache is updated each time a new prompt is submitted, and the full conversation history is saved to a file for each user. This allows the AI to generate responses that account for previous interactions.","title":"Conversation Caching"},{"location":"howItWorks/apigateway/#how-it-works","text":"When a prompt is received, the AI generates a response using the conversation history. The conversation is then stored in a file specific to the user ( conversationMemory/{username}_memory.txt ). The conversation history is formatted and used as context in future requests, allowing the AI to provide more coherent and contextually aware answers. Example of Conversation Cache Format : User: What is the meaning of life? AI: The meaning of life is to seek happiness, purpose, and connection with others. User: Can you elaborate on happiness? AI: Happiness is often found in meaningful relationships, personal growth, and living with purpose.","title":"How It Works:"},{"location":"howItWorks/apigateway/#file-storage","text":"The conversation is saved in a file named conversationMemory/{username}_memory.txt (where {username} is the unique identifier). The conversation cache is updated with each new user interaction.","title":"File Storage:"},{"location":"howItWorks/apigateway/#model-invocation","text":"The AI models are invoked through the Ollama class, which interacts with the Ollama service. This service manages the large language models (LLMs) and allows the API to generate text based on the provided prompt. llm = Ollama(model=model_name, base_url=\"http://ollama:11434\") Each time a prompt is received, the entire conversation history (stored in previous_convos ) is included as part of the prompt, enabling context-sensitive responses from the model. full_prompt = f'{previous_convos}\\nUser: {prompt_text}\\nAI:' return llm.invoke(\"Continue the conversation like a human\\n\" + full_prompt)","title":"Model Invocation"},{"location":"howItWorks/apigateway/#error-handling","text":"The application uses standard HTTP error codes for failure scenarios: 400 Bad Request : The request is malformed or missing required fields. 500 Internal Server Error : There is a problem on the server, such as an issue with the Ollama service or external dependencies. Example of error response for an unsuccessful transcription request: { \"error\": \"HTTP error occurred\", \"details\": \"Error details here\" }","title":"Error Handling"},{"location":"howItWorks/apigateway/#summary-of-available-endpoints","text":"Endpoint Method Description / GET Health check /api/prompt/ POST Generate prompt response /api/pull/ POST Pull a model from Ollama /api/delete/ POST Delete a model from Ollama /api/create/ POST Create a custom model /proxy-transcribe WebSocket Real-time audio transcription /proxy-send-transcription POST Send transcription request /audio/{username} GET Fetch user audio file This API allows seamless interaction with the Ollama LLM service while handling user-specific conversation histories and supporting real-time transcription of audio data.","title":"Summary of Available Endpoints"},{"location":"howItWorks/code/","text":"API Interaction Script This Python script demonstrates how to interact with an API gateway and a streaming service using the requests library. It includes functions for making GET and POST requests, and it tests the connection to the services defined by the API and streaming URLs. Environment Variables The script uses two environment variables to configure the ports for the API gateway and the streaming service: API_PORT : The port for the API gateway (default is 1200 ). STREAMING_PORT : The port for the streaming service (default is 1000 ). You can set these variables in your environment or use the defaults provided. Functions post_request(url, payload) Performs a POST request to the specified url with the given payload . Parameters url (str): The URL to which the request is sent. payload (dict): The JSON payload to be sent in the request body. Returns (dict): The JSON response from the server. get_request(url) Performs a GET request to the specified url . Parameters url (str): The URL to which the request is sent. Returns (dict): The JSON response from the server. Main Execution The script contains a main execution block that: Defines the URLs for the API gateway and the streaming service based on the configured ports. Tests the connection by sending GET requests to both services and prints their responses. Sends a POST request to the API gateway with a prompt payload, which includes: model : The name of the model (e.g., \"llama3.1\"). text : The text prompt (e.g., \"Hi!\"). Prints the response text from the POST request.","title":"API Interaction Script"},{"location":"howItWorks/code/#api-interaction-script","text":"This Python script demonstrates how to interact with an API gateway and a streaming service using the requests library. It includes functions for making GET and POST requests, and it tests the connection to the services defined by the API and streaming URLs.","title":"API Interaction Script"},{"location":"howItWorks/code/#environment-variables","text":"The script uses two environment variables to configure the ports for the API gateway and the streaming service: API_PORT : The port for the API gateway (default is 1200 ). STREAMING_PORT : The port for the streaming service (default is 1000 ). You can set these variables in your environment or use the defaults provided.","title":"Environment Variables"},{"location":"howItWorks/code/#functions","text":"","title":"Functions"},{"location":"howItWorks/code/#post_requesturl-payload","text":"Performs a POST request to the specified url with the given payload .","title":"post_request(url, payload)"},{"location":"howItWorks/code/#parameters","text":"url (str): The URL to which the request is sent. payload (dict): The JSON payload to be sent in the request body.","title":"Parameters"},{"location":"howItWorks/code/#returns","text":"(dict): The JSON response from the server.","title":"Returns"},{"location":"howItWorks/code/#get_requesturl","text":"Performs a GET request to the specified url .","title":"get_request(url)"},{"location":"howItWorks/code/#parameters_1","text":"url (str): The URL to which the request is sent.","title":"Parameters"},{"location":"howItWorks/code/#returns_1","text":"(dict): The JSON response from the server.","title":"Returns"},{"location":"howItWorks/code/#main-execution","text":"The script contains a main execution block that: Defines the URLs for the API gateway and the streaming service based on the configured ports. Tests the connection by sending GET requests to both services and prints their responses. Sends a POST request to the API gateway with a prompt payload, which includes: model : The name of the model (e.g., \"llama3.1\"). text : The text prompt (e.g., \"Hi!\"). Prints the response text from the POST request.","title":"Main Execution"},{"location":"howItWorks/dockerCompose/","text":"Docker Compose Setup for AudioStreamer Project This document describes the Docker Compose configuration for the AudioStreamer project, which consists of multiple services that work together to provide an audio streaming solution with GPU support. Services 1. Ollama Container Container Name : ollama Image : ollama/ollama Volumes : ./data/ollama:/root/.ollama : Mounts the local data/ollama directory to the container's .ollama directory. ./custom:/custom : Mounts the local custom directory to the container. Network : Connected to apinetwork . Environment Variables : OLLAMA_ORIGINS : Set to http://host.docker.internal . Deploy Configuration : Resources : Reservations : Devices : Driver : nvidia Count : all Capabilities : gpu (Enables GPU support for the container) Command : Runs serve and then list . 2. API Gateway Container Name : apigateway Build Context : ./api Ports : Exposes the API on ${API_PORT} . Volumes : ./api:/usr/src/app : Mounts the local api directory to the container's application directory. Depends On : ollama-container : Waits for this service to be ready. Network : Connected to apinetwork . Deploy Configuration : Resources : Reservations : Devices : Driver : nvidia Count : all Capabilities : gpu (Enables GPU support for the container) Health Check : Test : Uses curl to check if the service is responsive at http://ollama:11434 . Interval : 1 minute 30 seconds. Timeout : 30 seconds. Retries : 5 attempts. Start Period : 30 seconds. 3. Backend Code Container Name : backend Build Context : ./code Ports : Exposes the backend on ${BACKEND_PORT} . Volumes : ./code:/usr/src/app : Mounts the local code directory to the container's application directory. Depends On : apigateway : Requires the API gateway to be healthy before starting. streaming : Requires the streaming service to be healthy before starting. Network : Connected to apinetwork . Deploy Configuration : Resources : Reservations : Devices : Driver : nvidia Count : all Capabilities : gpu (Enables GPU support for the container) 4. Streaming Service Container Name : streaming Build Context : ./streaming Ports : Exposes the streaming service on ${STREAMING_PORT} . Volumes : ./streaming:/usr/src/app : Mounts the local streaming directory to the container's application directory. Depends On : ollama-container : Waits for this service to be ready. Network : Connected to apinetwork . Deploy Configuration : Resources : Reservations : Devices : Driver : nvidia Count : all Capabilities : gpu (Enables GPU support for the container) Health Check : Test : Uses curl to check if the service is responsive at http://ollama:11434 . Interval : 1 minute 30 seconds. Timeout : 30 seconds. Retries : 5 attempts. Start Period : 30 seconds. 5. Text-to-Speech (TTS) Service Container Name : tts Build Context : ./tts Ports : Exposes the TTS service on ${TEXT_TO_SPEECH} . Volumes : ./tts:/usr/src/app : Mounts the local tts directory to the container's application directory. Network : Connected to apinetwork . Deploy Configuration : Resources : Reservations : Devices : Driver : nvidia Count : all Capabilities : gpu (Enables GPU support for the container) Networks apinetwork : Driver : bridge This setup provides a comprehensive architecture for an audio streaming application utilizing Docker containers with GPU acceleration for relevant services.","title":"Docker Compose Setup for AudioStreamer Project"},{"location":"howItWorks/dockerCompose/#docker-compose-setup-for-audiostreamer-project","text":"This document describes the Docker Compose configuration for the AudioStreamer project, which consists of multiple services that work together to provide an audio streaming solution with GPU support.","title":"Docker Compose Setup for AudioStreamer Project"},{"location":"howItWorks/dockerCompose/#services","text":"","title":"Services"},{"location":"howItWorks/dockerCompose/#1-ollama-container","text":"Container Name : ollama Image : ollama/ollama Volumes : ./data/ollama:/root/.ollama : Mounts the local data/ollama directory to the container's .ollama directory. ./custom:/custom : Mounts the local custom directory to the container. Network : Connected to apinetwork . Environment Variables : OLLAMA_ORIGINS : Set to http://host.docker.internal . Deploy Configuration : Resources : Reservations : Devices : Driver : nvidia Count : all Capabilities : gpu (Enables GPU support for the container) Command : Runs serve and then list .","title":"1. Ollama Container"},{"location":"howItWorks/dockerCompose/#2-api-gateway","text":"Container Name : apigateway Build Context : ./api Ports : Exposes the API on ${API_PORT} . Volumes : ./api:/usr/src/app : Mounts the local api directory to the container's application directory. Depends On : ollama-container : Waits for this service to be ready. Network : Connected to apinetwork . Deploy Configuration : Resources : Reservations : Devices : Driver : nvidia Count : all Capabilities : gpu (Enables GPU support for the container) Health Check : Test : Uses curl to check if the service is responsive at http://ollama:11434 . Interval : 1 minute 30 seconds. Timeout : 30 seconds. Retries : 5 attempts. Start Period : 30 seconds.","title":"2. API Gateway"},{"location":"howItWorks/dockerCompose/#3-backend-code","text":"Container Name : backend Build Context : ./code Ports : Exposes the backend on ${BACKEND_PORT} . Volumes : ./code:/usr/src/app : Mounts the local code directory to the container's application directory. Depends On : apigateway : Requires the API gateway to be healthy before starting. streaming : Requires the streaming service to be healthy before starting. Network : Connected to apinetwork . Deploy Configuration : Resources : Reservations : Devices : Driver : nvidia Count : all Capabilities : gpu (Enables GPU support for the container)","title":"3. Backend Code"},{"location":"howItWorks/dockerCompose/#4-streaming-service","text":"Container Name : streaming Build Context : ./streaming Ports : Exposes the streaming service on ${STREAMING_PORT} . Volumes : ./streaming:/usr/src/app : Mounts the local streaming directory to the container's application directory. Depends On : ollama-container : Waits for this service to be ready. Network : Connected to apinetwork . Deploy Configuration : Resources : Reservations : Devices : Driver : nvidia Count : all Capabilities : gpu (Enables GPU support for the container) Health Check : Test : Uses curl to check if the service is responsive at http://ollama:11434 . Interval : 1 minute 30 seconds. Timeout : 30 seconds. Retries : 5 attempts. Start Period : 30 seconds.","title":"4. Streaming Service"},{"location":"howItWorks/dockerCompose/#5-text-to-speech-tts-service","text":"Container Name : tts Build Context : ./tts Ports : Exposes the TTS service on ${TEXT_TO_SPEECH} . Volumes : ./tts:/usr/src/app : Mounts the local tts directory to the container's application directory. Network : Connected to apinetwork . Deploy Configuration : Resources : Reservations : Devices : Driver : nvidia Count : all Capabilities : gpu (Enables GPU support for the container)","title":"5. Text-to-Speech (TTS) Service"},{"location":"howItWorks/dockerCompose/#networks","text":"apinetwork : Driver : bridge This setup provides a comprehensive architecture for an audio streaming application utilizing Docker containers with GPU acceleration for relevant services.","title":"Networks"},{"location":"howItWorks/streaming/","text":"Overview This FastAPI application provides real-time audio transcription using OpenAI's Whisper model. It allows clients to send audio data over HTTP or WebSocket, processes the audio, transcribes it, and optionally sends the transcription to an external API. Additionally, it supports generating audio from the transcription using a Text-to-Speech (TTS) model. Environment Variables The application utilizes the following environment variables: API_PORT : The port on which the FastAPI application will run (default: 1000 ). WHISPER_MODEL : The Whisper model to be loaded (default: base ). TEXT_TO_SPEECH : The port for the Text-to-Speech service (default: base ). You can set these variables in your environment or use a .env file. Endpoints GET / This endpoint performs a health check to verify that the server is running and the Whisper model is loaded. Response: 200 OK : Returns a message indicating the Whisper model in use. Example Response: { \"message\": \"The model is running on base model\" } POST /process-audio This HTTP endpoint accepts base64-encoded audio data, transcribes it using OpenAI's Whisper model, and returns the transcription. Request Body: { \"username\": \"user123\", \"audio\": \"base64_encoded_audio_data\" } username : A string representing the user's unique identifier. audio : The audio data in base64 encoding (e.g., a WebM or WAV file). Response: Returns the transcription result. { \"username\": \"user123\", \"transcription\": \"This is the transcribed text.\" } POST /send-transcription This endpoint triggers the sending of the transcribed text to an external API and sends the synthesized audio (via TTS) to the user. Request Body: { \"username\": \"user123\" } username : The unique identifier for the user. Response: { \"text\": \"This is the transcribed text.\", \"voice\": \"audio/user123_audio.wav\" } text : The response from the external API after sending the transcription. voice : The URL of the generated audio file from the Text-to-Speech service. GET /get-audio/{username} This endpoint serves the synthesized audio file (in WAV format) generated from the transcription. Parameters: username : The user's unique identifier. Response: 200 OK : Returns the audio file ( audio/{username}_audio.wav ). 404 Not Found : If the audio file does not exist. Functions process_wav_bytes(webm_bytes: bytes, sample_rate: int = 12000) Processes incoming WebM audio bytes to convert and load them as a waveform suitable for transcription. Parameters: webm_bytes (bytes): The audio bytes in WebM format. sample_rate (int): The sample rate for audio processing (default: 12000). Returns: The processed audio waveform, ready for transcription by Whisper. send_transcription_to_apigateway(file_path: str, username: str) Sends the transcription text to an external API for further processing. Parameters: file_path (str): The path to the transcription text file. username (str): The user's unique identifier. Returns: The JSON response from the external API, if successful. send_transcription_to_tts(text: str, username: str) Sends the transcription text to a Text-to-Speech (TTS) service and saves the resulting audio. Parameters: text (str): The transcription text to convert to speech. username (str): The user's unique identifier. Side Effects: Saves the resulting audio as a WAV file in the audio/ directory under the user's name ( audio/{username}_audio.wav ).","title":"Overview"},{"location":"howItWorks/streaming/#overview","text":"This FastAPI application provides real-time audio transcription using OpenAI's Whisper model. It allows clients to send audio data over HTTP or WebSocket, processes the audio, transcribes it, and optionally sends the transcription to an external API. Additionally, it supports generating audio from the transcription using a Text-to-Speech (TTS) model.","title":"Overview"},{"location":"howItWorks/streaming/#environment-variables","text":"The application utilizes the following environment variables: API_PORT : The port on which the FastAPI application will run (default: 1000 ). WHISPER_MODEL : The Whisper model to be loaded (default: base ). TEXT_TO_SPEECH : The port for the Text-to-Speech service (default: base ). You can set these variables in your environment or use a .env file.","title":"Environment Variables"},{"location":"howItWorks/streaming/#endpoints","text":"","title":"Endpoints"},{"location":"howItWorks/streaming/#get","text":"This endpoint performs a health check to verify that the server is running and the Whisper model is loaded.","title":"GET /"},{"location":"howItWorks/streaming/#response","text":"200 OK : Returns a message indicating the Whisper model in use. Example Response: { \"message\": \"The model is running on base model\" }","title":"Response:"},{"location":"howItWorks/streaming/#post-process-audio","text":"This HTTP endpoint accepts base64-encoded audio data, transcribes it using OpenAI's Whisper model, and returns the transcription.","title":"POST /process-audio"},{"location":"howItWorks/streaming/#request-body","text":"{ \"username\": \"user123\", \"audio\": \"base64_encoded_audio_data\" } username : A string representing the user's unique identifier. audio : The audio data in base64 encoding (e.g., a WebM or WAV file).","title":"Request Body:"},{"location":"howItWorks/streaming/#response_1","text":"Returns the transcription result. { \"username\": \"user123\", \"transcription\": \"This is the transcribed text.\" }","title":"Response:"},{"location":"howItWorks/streaming/#post-send-transcription","text":"This endpoint triggers the sending of the transcribed text to an external API and sends the synthesized audio (via TTS) to the user.","title":"POST /send-transcription"},{"location":"howItWorks/streaming/#request-body_1","text":"{ \"username\": \"user123\" } username : The unique identifier for the user.","title":"Request Body:"},{"location":"howItWorks/streaming/#response_2","text":"{ \"text\": \"This is the transcribed text.\", \"voice\": \"audio/user123_audio.wav\" } text : The response from the external API after sending the transcription. voice : The URL of the generated audio file from the Text-to-Speech service.","title":"Response:"},{"location":"howItWorks/streaming/#get-get-audiousername","text":"This endpoint serves the synthesized audio file (in WAV format) generated from the transcription.","title":"GET /get-audio/{username}"},{"location":"howItWorks/streaming/#parameters","text":"username : The user's unique identifier.","title":"Parameters:"},{"location":"howItWorks/streaming/#response_3","text":"200 OK : Returns the audio file ( audio/{username}_audio.wav ). 404 Not Found : If the audio file does not exist.","title":"Response:"},{"location":"howItWorks/streaming/#functions","text":"","title":"Functions"},{"location":"howItWorks/streaming/#process_wav_byteswebm_bytes-bytes-sample_rate-int-12000","text":"Processes incoming WebM audio bytes to convert and load them as a waveform suitable for transcription.","title":"process_wav_bytes(webm_bytes: bytes, sample_rate: int = 12000)"},{"location":"howItWorks/streaming/#parameters_1","text":"webm_bytes (bytes): The audio bytes in WebM format. sample_rate (int): The sample rate for audio processing (default: 12000).","title":"Parameters:"},{"location":"howItWorks/streaming/#returns","text":"The processed audio waveform, ready for transcription by Whisper.","title":"Returns:"},{"location":"howItWorks/streaming/#send_transcription_to_apigatewayfile_path-str-username-str","text":"Sends the transcription text to an external API for further processing.","title":"send_transcription_to_apigateway(file_path: str, username: str)"},{"location":"howItWorks/streaming/#parameters_2","text":"file_path (str): The path to the transcription text file. username (str): The user's unique identifier.","title":"Parameters:"},{"location":"howItWorks/streaming/#returns_1","text":"The JSON response from the external API, if successful.","title":"Returns:"},{"location":"howItWorks/streaming/#send_transcription_to_ttstext-str-username-str","text":"Sends the transcription text to a Text-to-Speech (TTS) service and saves the resulting audio.","title":"send_transcription_to_tts(text: str, username: str)"},{"location":"howItWorks/streaming/#parameters_3","text":"text (str): The transcription text to convert to speech. username (str): The user's unique identifier.","title":"Parameters:"},{"location":"howItWorks/streaming/#side-effects","text":"Saves the resulting audio as a WAV file in the audio/ directory under the user's name ( audio/{username}_audio.wav ).","title":"Side Effects:"},{"location":"howItWorks/tts/","text":"Overview This API provides functionality for converting text into speech. It allows users to send text data, which will then be processed using a text-to-speech (TTS) engine. The synthesized speech is returned as an audio file in WAV format. This API uses FastAPI as the framework and provides a simple interface for text-to-speech synthesis. Endpoints 1. Root Endpoint GET / The root endpoint provides a welcome message and basic information about the API. Response Status Code : 200 OK Response Body : json { \"message\": \"Welcome to the Text-to-Speech API\" } 2. Text-to-Speech Synthesis POST /synthesize This endpoint allows you to send a text string, and the API will return a WAV file of the synthesized speech. Request Body Content-Type : application/json Body : A JSON object with the following structure: json { \"text\": \"Your text here\" } text : (string) The text you want to convert to speech. Response Success : If the synthesis is successful, the response will contain the audio file as a WAV file. Status Code : 200 OK Response Body : The synthesized audio file will be returned as a binary stream with the following headers: Content-Type : audio/wav Content-Disposition : attachment; filename=\"synthesized_audio.wav\" Error : If there is an error (e.g., missing reference audio file or failure during synthesis), the response will contain an error message. Example Error Response : json { \"error\": \"Reference audio file not found.\" } Example Request curl -X 'POST'\\ 'http://{tts_url}/synthesize'\\ -H 'Content-Type: application/json'\\ -d '{ \"text\": \"Hello, world!\" }' Example Success Response The response will return a WAV file of the synthesized speech. Example Error Response If there is an issue (such as a missing reference audio file), the response will be: { \"error\": \"Reference audio file not found.\" } Configuration Environment Variables TEXT_TO_SPEECH : The port number on which the FastAPI server will run. If not set, the default value will be 1000 . Example: export TEXT_TO_SPEECH=8080 Logging The API includes logging at the DEBUG level. Logs will capture key events such as: Accessing endpoints. Command execution. Any errors that occur during synthesis. You can adjust the logging level as necessary to control verbosity. Error Handling The API includes basic error handling for issues such as: Missing reference audio file ( refaudio/audio.mp3 ). Issues during the synthesis process. File-related errors (e.g., file not found, file empty). In case of an error, a JSON response will be returned with the error message, and the status code will indicate failure (e.g., 500 Internal Server Error ). Local Development Command Line Synthesis ( f5-tts_infer-cli ) The actual synthesis of text to speech is handled by the f5-tts_infer-cli command-line tool. It is executed with the following parameters: -m E2-TTS : Specifies the TTS model to use. -r ./refaudio/audio.mp3 : Specifies the reference audio file used for synthesis. -s : An optional parameter for speaker selection. -t <text> : The text to be synthesized into speech. Ensure that the f5-tts_infer-cli tool is correctly installed and accessible from the system's PATH . The tool generates a WAV file ( infer_cli_out.wav ) that is returned to the client.","title":"Tts"},{"location":"howItWorks/tts/#overview","text":"This API provides functionality for converting text into speech. It allows users to send text data, which will then be processed using a text-to-speech (TTS) engine. The synthesized speech is returned as an audio file in WAV format. This API uses FastAPI as the framework and provides a simple interface for text-to-speech synthesis.","title":"Overview"},{"location":"howItWorks/tts/#endpoints","text":"","title":"Endpoints"},{"location":"howItWorks/tts/#1-root-endpoint","text":"","title":"1. Root Endpoint"},{"location":"howItWorks/tts/#get","text":"The root endpoint provides a welcome message and basic information about the API.","title":"GET /"},{"location":"howItWorks/tts/#response","text":"Status Code : 200 OK Response Body : json { \"message\": \"Welcome to the Text-to-Speech API\" }","title":"Response"},{"location":"howItWorks/tts/#2-text-to-speech-synthesis","text":"","title":"2. Text-to-Speech Synthesis"},{"location":"howItWorks/tts/#post-synthesize","text":"This endpoint allows you to send a text string, and the API will return a WAV file of the synthesized speech.","title":"POST /synthesize"},{"location":"howItWorks/tts/#request-body","text":"Content-Type : application/json Body : A JSON object with the following structure: json { \"text\": \"Your text here\" } text : (string) The text you want to convert to speech.","title":"Request Body"},{"location":"howItWorks/tts/#response_1","text":"Success : If the synthesis is successful, the response will contain the audio file as a WAV file. Status Code : 200 OK Response Body : The synthesized audio file will be returned as a binary stream with the following headers: Content-Type : audio/wav Content-Disposition : attachment; filename=\"synthesized_audio.wav\" Error : If there is an error (e.g., missing reference audio file or failure during synthesis), the response will contain an error message. Example Error Response : json { \"error\": \"Reference audio file not found.\" }","title":"Response"},{"location":"howItWorks/tts/#example-request","text":"curl -X 'POST'\\ 'http://{tts_url}/synthesize'\\ -H 'Content-Type: application/json'\\ -d '{ \"text\": \"Hello, world!\" }'","title":"Example Request"},{"location":"howItWorks/tts/#example-success-response","text":"The response will return a WAV file of the synthesized speech.","title":"Example Success Response"},{"location":"howItWorks/tts/#example-error-response","text":"If there is an issue (such as a missing reference audio file), the response will be: { \"error\": \"Reference audio file not found.\" }","title":"Example Error Response"},{"location":"howItWorks/tts/#configuration","text":"","title":"Configuration"},{"location":"howItWorks/tts/#environment-variables","text":"TEXT_TO_SPEECH : The port number on which the FastAPI server will run. If not set, the default value will be 1000 .","title":"Environment Variables"},{"location":"howItWorks/tts/#example","text":"export TEXT_TO_SPEECH=8080","title":"Example:"},{"location":"howItWorks/tts/#logging","text":"The API includes logging at the DEBUG level. Logs will capture key events such as: Accessing endpoints. Command execution. Any errors that occur during synthesis. You can adjust the logging level as necessary to control verbosity.","title":"Logging"},{"location":"howItWorks/tts/#error-handling","text":"The API includes basic error handling for issues such as: Missing reference audio file ( refaudio/audio.mp3 ). Issues during the synthesis process. File-related errors (e.g., file not found, file empty). In case of an error, a JSON response will be returned with the error message, and the status code will indicate failure (e.g., 500 Internal Server Error ).","title":"Error Handling"},{"location":"howItWorks/tts/#local-development","text":"","title":"Local Development"},{"location":"howItWorks/tts/#command-line-synthesis-f5-tts_infer-cli","text":"The actual synthesis of text to speech is handled by the f5-tts_infer-cli command-line tool. It is executed with the following parameters: -m E2-TTS : Specifies the TTS model to use. -r ./refaudio/audio.mp3 : Specifies the reference audio file used for synthesis. -s : An optional parameter for speaker selection. -t <text> : The text to be synthesized into speech. Ensure that the f5-tts_infer-cli tool is correctly installed and accessible from the system's PATH . The tool generates a WAV file ( infer_cli_out.wav ) that is returned to the client.","title":"Command Line Synthesis (f5-tts_infer-cli)"},{"location":"nginx/nginxCode/","text":"Code for the nginx setup Structure . \u251c\u2500\u2500 fastcgi.conf \ud83d\udcc4 \u251c\u2500\u2500 fastcgi_params \ud83d\udcc4 \u251c\u2500\u2500 koi-utf \ud83d\udcc4 \u251c\u2500\u2500 koi-win \ud83d\udcc4 \u251c\u2500\u2500 mime.types \ud83d\udcc4 \u251c\u2500\u2500 nginx.conf \ud83d\udcc4 \u251c\u2500\u2500 proxy_params \ud83d\udcc4 \u251c\u2500\u2500 scgi_params \ud83d\udcc4 \u251c\u2500\u2500 sites-available \ud83d\udcc2 \u2502 \u2514\u2500\u2500 aifrontend.com \ud83d\udcc4 \u251c\u2500\u2500 sites-enabled \ud83d\udcc2 \u2502 \u2514\u2500\u2500 aifrontend.com -> /etc/nginx/sites-available/aifrontend.com \ud83d\udcc4 \u251c\u2500\u2500 ssl \ud83d\udcc2 \u2502 \u251c\u2500\u2500 servercert.pem \ud83d\udcc4 \u2502 \u2514\u2500\u2500 serverkey.pem \ud83d\udcc4 \u251c\u2500\u2500 uwsgi_params \ud83d\udcc4 \u2514\u2500\u2500 win-utf \ud83d\udcc4 nginx.conf Creating the configuration under which nginx will run worker_processes 1; events { worker_connections 1024; } http { include mime.types; include sites-enabled/*; server_names_hash_bucket_size 64; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server { listen 8222; server_name localhost; location / { root /usr/share/nginx/html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } } Proxy params Creating proxy parameters proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; Sites available Creating connection inside the proxies. Here you should also create a certificate, that you put in the ssl folder. (I've used mkcert) server { listen 443 ssl; listen [::]:443 ssl; server_name {your_ip}; ssl_certificate /etc/nginx/ssl/servercert.pem; ssl_certificate_key /etc/nginx/ssl/serverkey.pem; ssl_protocols TLSv1.2 TLSv1.3; ssl_ciphers 'TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256'; location / { proxy_pass https://localhost:8888; include proxy_params; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection 'upgrade'; proxy_http_version 1.1; } location /api/ { proxy_pass http://localhost:8502/; include proxy_params; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection 'upgrade'; proxy_http_version 1.1; } location /ollama/ { proxy_pass http://localhost:11434/; include proxy_params; } location /streaming/ { proxy_pass http://localhost:8503/; include proxy_params; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection 'upgrade'; proxy_http_version 1.1; } location /tts/ { proxy_pass http://localhost:8504/; include proxy_params; } } server { listen 80; listen [::]:80; server_name {your_ip}; # Redirect HTTP to HTTPS return 301 https://$host$request_uri; } When it's done then run: sudo ln -s /etc/nginx/sites-available/aifrontend.com /etc/nginx/sites-enabled/ Then restart nginx: sudo systemctl restart nginx","title":"Code for the nginx setup"},{"location":"nginx/nginxCode/#code-for-the-nginx-setup","text":"","title":"Code for the nginx setup"},{"location":"nginx/nginxCode/#structure","text":". \u251c\u2500\u2500 fastcgi.conf \ud83d\udcc4 \u251c\u2500\u2500 fastcgi_params \ud83d\udcc4 \u251c\u2500\u2500 koi-utf \ud83d\udcc4 \u251c\u2500\u2500 koi-win \ud83d\udcc4 \u251c\u2500\u2500 mime.types \ud83d\udcc4 \u251c\u2500\u2500 nginx.conf \ud83d\udcc4 \u251c\u2500\u2500 proxy_params \ud83d\udcc4 \u251c\u2500\u2500 scgi_params \ud83d\udcc4 \u251c\u2500\u2500 sites-available \ud83d\udcc2 \u2502 \u2514\u2500\u2500 aifrontend.com \ud83d\udcc4 \u251c\u2500\u2500 sites-enabled \ud83d\udcc2 \u2502 \u2514\u2500\u2500 aifrontend.com -> /etc/nginx/sites-available/aifrontend.com \ud83d\udcc4 \u251c\u2500\u2500 ssl \ud83d\udcc2 \u2502 \u251c\u2500\u2500 servercert.pem \ud83d\udcc4 \u2502 \u2514\u2500\u2500 serverkey.pem \ud83d\udcc4 \u251c\u2500\u2500 uwsgi_params \ud83d\udcc4 \u2514\u2500\u2500 win-utf \ud83d\udcc4","title":"Structure"},{"location":"nginx/nginxCode/#nginxconf","text":"Creating the configuration under which nginx will run worker_processes 1; events { worker_connections 1024; } http { include mime.types; include sites-enabled/*; server_names_hash_bucket_size 64; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server { listen 8222; server_name localhost; location / { root /usr/share/nginx/html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } }","title":"nginx.conf"},{"location":"nginx/nginxCode/#proxy-params","text":"Creating proxy parameters proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme;","title":"Proxy params"},{"location":"nginx/nginxCode/#sites-available","text":"Creating connection inside the proxies. Here you should also create a certificate, that you put in the ssl folder. (I've used mkcert) server { listen 443 ssl; listen [::]:443 ssl; server_name {your_ip}; ssl_certificate /etc/nginx/ssl/servercert.pem; ssl_certificate_key /etc/nginx/ssl/serverkey.pem; ssl_protocols TLSv1.2 TLSv1.3; ssl_ciphers 'TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256'; location / { proxy_pass https://localhost:8888; include proxy_params; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection 'upgrade'; proxy_http_version 1.1; } location /api/ { proxy_pass http://localhost:8502/; include proxy_params; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection 'upgrade'; proxy_http_version 1.1; } location /ollama/ { proxy_pass http://localhost:11434/; include proxy_params; } location /streaming/ { proxy_pass http://localhost:8503/; include proxy_params; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection 'upgrade'; proxy_http_version 1.1; } location /tts/ { proxy_pass http://localhost:8504/; include proxy_params; } } server { listen 80; listen [::]:80; server_name {your_ip}; # Redirect HTTP to HTTPS return 301 https://$host$request_uri; } When it's done then run: sudo ln -s /etc/nginx/sites-available/aifrontend.com /etc/nginx/sites-enabled/ Then restart nginx: sudo systemctl restart nginx","title":"Sites available"},{"location":"nginx/nginxConf/","text":"NGINX Configuration Explanation This configuration file defines basic settings for an NGINX web server, including worker processes, event handling, and HTTP server settings. It listens on port 8222 and serves static files. Global Settings worker_processes 1; Description : Specifies the number of worker processes NGINX should spawn to handle incoming requests. Here, it's set to 1 , meaning one worker process will handle all the connections. events {} Description : Defines how NGINX will handle incoming connections in an asynchronous, non-blocking manner. worker_connections 1024; Description : Sets the maximum number of simultaneous connections that each worker process can handle. In this case, it's set to 1024 , meaning each worker can handle up to 1024 connections concurrently. HTTP Settings The http {} block defines the behavior of NGINX when handling HTTP requests. include mime.types; Description : This line includes the mime.types file, which helps NGINX recognize file types based on file extensions. It determines the correct Content-Type header to serve for each file type. include sites-enabled/*; Description : This includes any additional site-specific configurations stored in the sites-enabled directory. It allows for modular site configuration (like virtual hosts). server_names_hash_bucket_size 64; Description : Sets the hash bucket size for server names. This is necessary when dealing with long or numerous domain names. A value of 64 is a common setting, which ensures optimal performance when hashing server names. default_type application/octet-stream; Description : Defines the default MIME type when NGINX can't determine the file type. Here, it defaults to application/octet-stream , which is a generic binary stream type. sendfile on; Description : Enables the sendfile() system call for sending files. This improves performance when serving large files, as it avoids copying the file data between user space and kernel space. keepalive_timeout 65; Description : Sets the timeout for keeping a connection open after completing a request. In this case, it is set to 65 seconds. If the client doesn't send any data within this period, the connection is closed. Server Block The server {} block defines the behavior for a specific server or virtual host. listen 8222; Description : Instructs NGINX to listen for incoming requests on port 8222 . This is the port on which the server will accept connections. server_name localhost; Description : Defines the server name (or domain) for this block. In this case, it's set to localhost , meaning it will respond to requests sent to localhost or 127.0.0.1 . Location Block The location {} block defines how requests for specific URIs should be handled. location / {} Description : Defines how to handle requests to the root URL ( / ) of the server. root /usr/share/nginx/html; Description : Specifies the root directory for this location. Files will be served from /usr/share/nginx/html when clients request URLs under / . index index.html index.htm; Description : Defines the default index files for this location. If a client requests the root URL ( / ), NGINX will first look for index.html , and if not found, it will look for index.htm to serve as the default file. Error Handling error_page 500 502 503 504 /50x.html; Description : Specifies a custom error page to be served for specific HTTP errors ( 500 , 502 , 503 , 504 ). When these errors occur, NGINX will display the 50x.html page. location = /50x.html {} Description : Defines how to serve the custom error page. root /usr/share/nginx/html; Description : Specifies the location of the 50x.html file. It will be served from the /usr/share/nginx/html directory. Summary This NGINX configuration sets up a basic web server with the following characteristics: - One worker process handling up to 1024 concurrent connections . - HTTP server listening on port 8222 . - Serving static files from the /usr/share/nginx/html directory. - A default error page for common server errors ( 500 , 502 , 503 , 504 ).","title":"NGINX Configuration Explanation"},{"location":"nginx/nginxConf/#nginx-configuration-explanation","text":"This configuration file defines basic settings for an NGINX web server, including worker processes, event handling, and HTTP server settings. It listens on port 8222 and serves static files.","title":"NGINX Configuration Explanation"},{"location":"nginx/nginxConf/#global-settings","text":"","title":"Global Settings"},{"location":"nginx/nginxConf/#worker_processes-1","text":"Description : Specifies the number of worker processes NGINX should spawn to handle incoming requests. Here, it's set to 1 , meaning one worker process will handle all the connections.","title":"worker_processes 1;"},{"location":"nginx/nginxConf/#events","text":"Description : Defines how NGINX will handle incoming connections in an asynchronous, non-blocking manner.","title":"events {}"},{"location":"nginx/nginxConf/#worker_connections-1024","text":"Description : Sets the maximum number of simultaneous connections that each worker process can handle. In this case, it's set to 1024 , meaning each worker can handle up to 1024 connections concurrently.","title":"worker_connections 1024;"},{"location":"nginx/nginxConf/#http-settings","text":"The http {} block defines the behavior of NGINX when handling HTTP requests.","title":"HTTP Settings"},{"location":"nginx/nginxConf/#include-mimetypes","text":"Description : This line includes the mime.types file, which helps NGINX recognize file types based on file extensions. It determines the correct Content-Type header to serve for each file type.","title":"include mime.types;"},{"location":"nginx/nginxConf/#include-sites-enabled","text":"Description : This includes any additional site-specific configurations stored in the sites-enabled directory. It allows for modular site configuration (like virtual hosts).","title":"include sites-enabled/*;"},{"location":"nginx/nginxConf/#server_names_hash_bucket_size-64","text":"Description : Sets the hash bucket size for server names. This is necessary when dealing with long or numerous domain names. A value of 64 is a common setting, which ensures optimal performance when hashing server names.","title":"server_names_hash_bucket_size 64;"},{"location":"nginx/nginxConf/#default_type-applicationoctet-stream","text":"Description : Defines the default MIME type when NGINX can't determine the file type. Here, it defaults to application/octet-stream , which is a generic binary stream type.","title":"default_type application/octet-stream;"},{"location":"nginx/nginxConf/#sendfile-on","text":"Description : Enables the sendfile() system call for sending files. This improves performance when serving large files, as it avoids copying the file data between user space and kernel space.","title":"sendfile on;"},{"location":"nginx/nginxConf/#keepalive_timeout-65","text":"Description : Sets the timeout for keeping a connection open after completing a request. In this case, it is set to 65 seconds. If the client doesn't send any data within this period, the connection is closed.","title":"keepalive_timeout 65;"},{"location":"nginx/nginxConf/#server-block","text":"The server {} block defines the behavior for a specific server or virtual host.","title":"Server Block"},{"location":"nginx/nginxConf/#listen-8222","text":"Description : Instructs NGINX to listen for incoming requests on port 8222 . This is the port on which the server will accept connections.","title":"listen 8222;"},{"location":"nginx/nginxConf/#server_name-localhost","text":"Description : Defines the server name (or domain) for this block. In this case, it's set to localhost , meaning it will respond to requests sent to localhost or 127.0.0.1 .","title":"server_name localhost;"},{"location":"nginx/nginxConf/#location-block","text":"The location {} block defines how requests for specific URIs should be handled.","title":"Location Block"},{"location":"nginx/nginxConf/#location","text":"Description : Defines how to handle requests to the root URL ( / ) of the server.","title":"location / {}"},{"location":"nginx/nginxConf/#root-usrsharenginxhtml","text":"Description : Specifies the root directory for this location. Files will be served from /usr/share/nginx/html when clients request URLs under / .","title":"root /usr/share/nginx/html;"},{"location":"nginx/nginxConf/#index-indexhtml-indexhtm","text":"Description : Defines the default index files for this location. If a client requests the root URL ( / ), NGINX will first look for index.html , and if not found, it will look for index.htm to serve as the default file.","title":"index index.html index.htm;"},{"location":"nginx/nginxConf/#error-handling","text":"","title":"Error Handling"},{"location":"nginx/nginxConf/#error_page-500-502-503-504-50xhtml","text":"Description : Specifies a custom error page to be served for specific HTTP errors ( 500 , 502 , 503 , 504 ). When these errors occur, NGINX will display the 50x.html page.","title":"error_page 500 502 503 504 /50x.html;"},{"location":"nginx/nginxConf/#location-50xhtml","text":"Description : Defines how to serve the custom error page.","title":"location = /50x.html {}"},{"location":"nginx/nginxConf/#root-usrsharenginxhtml_1","text":"Description : Specifies the location of the 50x.html file. It will be served from the /usr/share/nginx/html directory.","title":"root /usr/share/nginx/html;"},{"location":"nginx/nginxConf/#summary","text":"This NGINX configuration sets up a basic web server with the following characteristics: - One worker process handling up to 1024 concurrent connections . - HTTP server listening on port 8222 . - Serving static files from the /usr/share/nginx/html directory. - A default error page for common server errors ( 500 , 502 , 503 , 504 ).","title":"Summary"},{"location":"nginx/proxyConf/","text":"NGINX SSL and Proxy Configuration Explanation This NGINX configuration defines two server blocks: one for handling HTTPS requests and the other for redirecting HTTP requests to HTTPS. It also sets up reverse proxying to multiple backend services based on different URL paths. First Server Block (HTTPS) This block listens for secure HTTPS connections on port 443 using SSL/TLS. listen 443 ssl; and listen [::]:443 ssl; Description : This directive tells NGINX to listen for incoming connections on port 443 for both IPv4 and IPv6, using SSL for encryption. server_name your_ip; Description : Specifies the server name (or IP address) that this server block will respond to. In this case, it listens for requests sent to the IP your_ip . SSL Settings The following directives configure SSL/TLS for secure communication. ssl_certificate /etc/nginx/ssl/servercert.pem; Description : Specifies the path to the SSL certificate file used to encrypt communication. ssl_certificate_key /etc/nginx/ssl/serverkey.pem; Description : Specifies the path to the private key file corresponding to the SSL certificate. This key is used to decrypt incoming encrypted connections. ssl_protocols TLSv1.2 TLSv1.3; Description : Defines the TLS protocols that are allowed. In this case, only TLS versions 1.2 and 1.3 are supported, which are considered secure. ssl_ciphers 'TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256'; Description : Defines the allowed cipher suites for SSL/TLS connections. The specified ciphers offer strong encryption while ensuring compatibility with modern clients. Proxy Configuration The following location blocks define how requests for different paths should be proxied to various backend services. location / {} Description : For requests to the root path ( / ), this block proxies traffic to an internal service running on localhost:{your_frontend_port} . proxy_pass https://localhost:{your_frontend_port}; : Proxies the request to an HTTPS backend running on port {your_frontend_port} . include proxy_params; : Includes additional parameters for proxying, such as forwarding the original client IP and headers. proxy_set_header Upgrade $http_upgrade; and proxy_set_header Connection 'upgrade'; : These headers are necessary for handling WebSocket upgrades. proxy_http_version 1.1; : Forces the use of HTTP/1.1 to support WebSocket connections. location /api/ {} Description : Proxies requests for paths starting with /api/ to an internal service running on localhost:{your_api_port} . proxy_pass http://localhost:{your_api_port}/; : Proxies HTTP traffic to the backend service. location /ollama/ {} Description : Proxies requests for paths starting with /ollama/ to localhost:11434 . proxy_pass http://localhost:11434/; : Proxies HTTP traffic to the backend service. location /streaming/ {} Description : Proxies requests for paths starting with /streaming/ to a service running on localhost:{your_streaming_port} . proxy_set_header Upgrade $http_upgrade; and proxy_set_header Connection 'upgrade'; : These headers are necessary for WebSocket connections. proxy_http_version 1.1; : Forces HTTP/1.1 for WebSocket support. location /tts/ {} Description : Proxies requests for paths starting with /tts/ to a service on localhost:{your_tts_port} . proxy_pass http://localhost:{your_tts_port}/; : Proxies HTTP traffic to the backend service. Second Server Block (HTTP to HTTPS Redirect) This server block handles HTTP requests and redirects them to HTTPS. listen 80; and listen [::]:80; Description : This tells NGINX to listen for incoming HTTP connections on port 80 (both IPv4 and IPv6). server_name your_ip; Description : The server name for this block, matching the IP address your_ip . HTTP to HTTPS Redirect return 301 https://$host$request_uri; Description : Redirects all incoming HTTP requests to HTTPS. The 301 status code indicates a permanent redirect. $host : Preserves the original hostname from the request. $request_uri : Includes the original requested URI (path and query string). This ensures that any HTTP request is automatically redirected to its HTTPS equivalent. Summary This NGINX configuration handles: - HTTPS traffic (on port 443 ) with SSL/TLS encryption, including the following reverse proxy setups: - Root path ( / ) proxies to localhost:{your_frontend_port} (with WebSocket support). - /api/ , /ollama/ , /streaming/ , and /tts/ paths proxy to their respective backend services. - HTTP traffic (on port 80 ) is redirected to the HTTPS version of the same request, ensuring all traffic is secured. SSL and Proxy Highlights: SSL encryption is enabled using TLSv1.2 and TLSv1.3 protocols with strong cipher suites. Requests are proxied to various internal services based on the path, with WebSocket support enabled for certain paths ( / , /streaming/ ).","title":"NGINX SSL and Proxy Configuration Explanation"},{"location":"nginx/proxyConf/#nginx-ssl-and-proxy-configuration-explanation","text":"This NGINX configuration defines two server blocks: one for handling HTTPS requests and the other for redirecting HTTP requests to HTTPS. It also sets up reverse proxying to multiple backend services based on different URL paths.","title":"NGINX SSL and Proxy Configuration Explanation"},{"location":"nginx/proxyConf/#first-server-block-https","text":"This block listens for secure HTTPS connections on port 443 using SSL/TLS.","title":"First Server Block (HTTPS)"},{"location":"nginx/proxyConf/#listen-443-ssl-and-listen-443-ssl","text":"Description : This directive tells NGINX to listen for incoming connections on port 443 for both IPv4 and IPv6, using SSL for encryption.","title":"listen 443 ssl; and listen [::]:443 ssl;"},{"location":"nginx/proxyConf/#server_name-your_ip","text":"Description : Specifies the server name (or IP address) that this server block will respond to. In this case, it listens for requests sent to the IP your_ip .","title":"server_name your_ip;"},{"location":"nginx/proxyConf/#ssl-settings","text":"The following directives configure SSL/TLS for secure communication.","title":"SSL Settings"},{"location":"nginx/proxyConf/#ssl_certificate-etcnginxsslservercertpem","text":"Description : Specifies the path to the SSL certificate file used to encrypt communication.","title":"ssl_certificate /etc/nginx/ssl/servercert.pem;"},{"location":"nginx/proxyConf/#ssl_certificate_key-etcnginxsslserverkeypem","text":"Description : Specifies the path to the private key file corresponding to the SSL certificate. This key is used to decrypt incoming encrypted connections.","title":"ssl_certificate_key /etc/nginx/ssl/serverkey.pem;"},{"location":"nginx/proxyConf/#ssl_protocols-tlsv12-tlsv13","text":"Description : Defines the TLS protocols that are allowed. In this case, only TLS versions 1.2 and 1.3 are supported, which are considered secure.","title":"ssl_protocols TLSv1.2 TLSv1.3;"},{"location":"nginx/proxyConf/#ssl_ciphers-tls_aes_128_gcm_sha256tls_aes_256_gcm_sha384tls_ecdhe_rsa_with_aes_128_gcm_sha256","text":"Description : Defines the allowed cipher suites for SSL/TLS connections. The specified ciphers offer strong encryption while ensuring compatibility with modern clients.","title":"ssl_ciphers 'TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256';"},{"location":"nginx/proxyConf/#proxy-configuration","text":"The following location blocks define how requests for different paths should be proxied to various backend services.","title":"Proxy Configuration"},{"location":"nginx/proxyConf/#location","text":"Description : For requests to the root path ( / ), this block proxies traffic to an internal service running on localhost:{your_frontend_port} . proxy_pass https://localhost:{your_frontend_port}; : Proxies the request to an HTTPS backend running on port {your_frontend_port} . include proxy_params; : Includes additional parameters for proxying, such as forwarding the original client IP and headers. proxy_set_header Upgrade $http_upgrade; and proxy_set_header Connection 'upgrade'; : These headers are necessary for handling WebSocket upgrades. proxy_http_version 1.1; : Forces the use of HTTP/1.1 to support WebSocket connections.","title":"location / {}"},{"location":"nginx/proxyConf/#location-api","text":"Description : Proxies requests for paths starting with /api/ to an internal service running on localhost:{your_api_port} . proxy_pass http://localhost:{your_api_port}/; : Proxies HTTP traffic to the backend service.","title":"location /api/ {}"},{"location":"nginx/proxyConf/#location-ollama","text":"Description : Proxies requests for paths starting with /ollama/ to localhost:11434 . proxy_pass http://localhost:11434/; : Proxies HTTP traffic to the backend service.","title":"location /ollama/ {}"},{"location":"nginx/proxyConf/#location-streaming","text":"Description : Proxies requests for paths starting with /streaming/ to a service running on localhost:{your_streaming_port} . proxy_set_header Upgrade $http_upgrade; and proxy_set_header Connection 'upgrade'; : These headers are necessary for WebSocket connections. proxy_http_version 1.1; : Forces HTTP/1.1 for WebSocket support.","title":"location /streaming/ {}"},{"location":"nginx/proxyConf/#location-tts","text":"Description : Proxies requests for paths starting with /tts/ to a service on localhost:{your_tts_port} . proxy_pass http://localhost:{your_tts_port}/; : Proxies HTTP traffic to the backend service.","title":"location /tts/ {}"},{"location":"nginx/proxyConf/#second-server-block-http-to-https-redirect","text":"This server block handles HTTP requests and redirects them to HTTPS.","title":"Second Server Block (HTTP to HTTPS Redirect)"},{"location":"nginx/proxyConf/#listen-80-and-listen-80","text":"Description : This tells NGINX to listen for incoming HTTP connections on port 80 (both IPv4 and IPv6).","title":"listen 80; and listen [::]:80;"},{"location":"nginx/proxyConf/#server_name-your_ip_1","text":"Description : The server name for this block, matching the IP address your_ip .","title":"server_name your_ip;"},{"location":"nginx/proxyConf/#http-to-https-redirect","text":"","title":"HTTP to HTTPS Redirect"},{"location":"nginx/proxyConf/#return-301-httpshostrequest_uri","text":"Description : Redirects all incoming HTTP requests to HTTPS. The 301 status code indicates a permanent redirect. $host : Preserves the original hostname from the request. $request_uri : Includes the original requested URI (path and query string). This ensures that any HTTP request is automatically redirected to its HTTPS equivalent.","title":"return 301 https://$host$request_uri;"},{"location":"nginx/proxyConf/#summary","text":"This NGINX configuration handles: - HTTPS traffic (on port 443 ) with SSL/TLS encryption, including the following reverse proxy setups: - Root path ( / ) proxies to localhost:{your_frontend_port} (with WebSocket support). - /api/ , /ollama/ , /streaming/ , and /tts/ paths proxy to their respective backend services. - HTTP traffic (on port 80 ) is redirected to the HTTPS version of the same request, ensuring all traffic is secured.","title":"Summary"},{"location":"nginx/proxyConf/#ssl-and-proxy-highlights","text":"SSL encryption is enabled using TLSv1.2 and TLSv1.3 protocols with strong cipher suites. Requests are proxied to various internal services based on the path, with WebSocket support enabled for certain paths ( / , /streaming/ ).","title":"SSL and Proxy Highlights:"},{"location":"nginx/proxyParams/","text":"Proxy Header Configuration Explanation This section contains directives for setting HTTP headers in NGINX when acting as a reverse proxy. These headers are used to pass information about the client request and the original protocol used, ensuring that the upstream server can correctly interpret the request. proxy_set_header Host $http_host; Description : This directive sets the Host header in the request that is forwarded to the upstream server. $http_host : This variable represents the original Host header sent by the client in the HTTP request. Purpose : Ensures that the upstream server sees the original Host header, allowing it to properly respond based on the requested domain or subdomain. proxy_set_header X-Real-IP $remote_addr; Description : This directive forwards the client's original IP address to the upstream server by setting the X-Real-IP header. $remote_addr : This variable contains the IP address of the client making the request. Purpose : Passes the client's real IP address to the upstream server, which may be useful for logging, analytics, or security purposes, especially when NGINX is behind a load balancer or proxy. proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; Description : This directive sets the X-Forwarded-For header, which is a comma-separated list of IP addresses representing the chain of proxies that forwarded the request. $proxy_add_x_forwarded_for : This variable appends the client's IP address ( $remote_addr ) to the existing X-Forwarded-For header, or creates it if it does not exist. Purpose : Allows the upstream server to trace the entire path of the client request, especially when there are multiple proxies involved. This is crucial for logging the real client IP in multi-layered proxy setups. proxy_set_header X-Forwarded-Proto $scheme; Description : This directive sets the X-Forwarded-Proto header, which indicates the protocol (HTTP or HTTPS) that was used by the client to connect to the original NGINX server. $scheme : This variable represents the protocol used in the original request, either http or https . Purpose : Communicates to the upstream server whether the original client connection was secured (HTTPS) or not (HTTP). This is useful for generating correct links or handling mixed-content issues. Summary These proxy_set_header directives are used in reverse proxy configurations to pass relevant client information from the NGINX proxy to the upstream server. The headers help the upstream server correctly handle: - The requested domain ( Host header), - The real client IP address ( X-Real-IP , X-Forwarded-For ), - And the protocol used ( X-Forwarded-Proto ). This setup ensures that the upstream server has all the necessary context to handle requests properly in a proxy or load-balanced environment.","title":"Proxy Header Configuration Explanation"},{"location":"nginx/proxyParams/#proxy-header-configuration-explanation","text":"This section contains directives for setting HTTP headers in NGINX when acting as a reverse proxy. These headers are used to pass information about the client request and the original protocol used, ensuring that the upstream server can correctly interpret the request.","title":"Proxy Header Configuration Explanation"},{"location":"nginx/proxyParams/#proxy_set_header-host-http_host","text":"Description : This directive sets the Host header in the request that is forwarded to the upstream server. $http_host : This variable represents the original Host header sent by the client in the HTTP request. Purpose : Ensures that the upstream server sees the original Host header, allowing it to properly respond based on the requested domain or subdomain.","title":"proxy_set_header Host $http_host;"},{"location":"nginx/proxyParams/#proxy_set_header-x-real-ip-remote_addr","text":"Description : This directive forwards the client's original IP address to the upstream server by setting the X-Real-IP header. $remote_addr : This variable contains the IP address of the client making the request. Purpose : Passes the client's real IP address to the upstream server, which may be useful for logging, analytics, or security purposes, especially when NGINX is behind a load balancer or proxy.","title":"proxy_set_header X-Real-IP $remote_addr;"},{"location":"nginx/proxyParams/#proxy_set_header-x-forwarded-for-proxy_add_x_forwarded_for","text":"Description : This directive sets the X-Forwarded-For header, which is a comma-separated list of IP addresses representing the chain of proxies that forwarded the request. $proxy_add_x_forwarded_for : This variable appends the client's IP address ( $remote_addr ) to the existing X-Forwarded-For header, or creates it if it does not exist. Purpose : Allows the upstream server to trace the entire path of the client request, especially when there are multiple proxies involved. This is crucial for logging the real client IP in multi-layered proxy setups.","title":"proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;"},{"location":"nginx/proxyParams/#proxy_set_header-x-forwarded-proto-scheme","text":"Description : This directive sets the X-Forwarded-Proto header, which indicates the protocol (HTTP or HTTPS) that was used by the client to connect to the original NGINX server. $scheme : This variable represents the protocol used in the original request, either http or https . Purpose : Communicates to the upstream server whether the original client connection was secured (HTTPS) or not (HTTP). This is useful for generating correct links or handling mixed-content issues.","title":"proxy_set_header X-Forwarded-Proto $scheme;"},{"location":"nginx/proxyParams/#summary","text":"These proxy_set_header directives are used in reverse proxy configurations to pass relevant client information from the NGINX proxy to the upstream server. The headers help the upstream server correctly handle: - The requested domain ( Host header), - The real client IP address ( X-Real-IP , X-Forwarded-For ), - And the protocol used ( X-Forwarded-Proto ). This setup ensures that the upstream server has all the necessary context to handle requests properly in a proxy or load-balanced environment.","title":"Summary"}]}